{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![logo](../imgs/MLU_Logo.png)\n",
    "\n",
    "---\n",
    "\n",
    "# EndToEnd MLOps Pipeline\n",
    "\n",
    "Now, it is time for automating the ML pipeline using the MLOps environment.\n",
    "\n",
    "First, we will collect the data sets, traning parameters, deploying parameters, and then zip them into a file. This zip file has the following structure:\n",
    " - trainingjob.json (Sagemaker training job descriptor)\n",
    " - environment.json (instructions to the environment of how to deploy and prepare the endpoints)\n",
    " \n",
    "Then by calling the `s3.put_object` command, we put the zipped file in an S3 bucket. At the same time, [CodePipeline](https://us-west-2.console.aws.amazon.com/codesuite/codepipeline/pipelines/iris-model-pipeline/view?region=us-west-2) monitors that bucket, and will start a job once the zipped file is detected. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import time\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import io\n",
    "import zipfile\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the MLOps Pipeline\n",
    "\n",
    "Once the S3 bucket gets the new data, the MLOps pipeline will be kicked off automatically. The pipeline contains the following functions:\n",
    "- `Source`: action (\"putting a `.zip` file in S3\") that you want to capture, in order to automaticlly start this CodePipeline\n",
    "- `ProcessRequest`: AWS Lambda function \"[mlops-op-process-request](https://us-west-2.console.aws.amazon.com/lambda/home?region=us-west-2#/functions/mlops-op-process-request?tab=code)\" that processes the request and prepares the CloudFormation templates for training;\n",
    "- `Train`: AWS **CloudFormation** stack to train (see in below picture);\n",
    "- `DeployDev`: AWS **Cloud Formation** stack to deploy trained model to **Development** environment (see in below picture);\n",
    "- `DeployApproval`: An **action you need** to take manually in either by clicking `DeployApproval` in Codepipeline or with code approval (shown in next notebook;\n",
    "- `DeployProd`: an AWS Cloud Formation stack to deploy trained model to **Production** environment (see in below picture);\n",
    "\n",
    "The overall pipeline will take around **20 minutes** to build. The finished pipeline is shown below (screenshot from CodePipeline):\n",
    "\n",
    "<img src=\"../imgs/codepipeline.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "## UserName\n",
    " \n",
    "The only parameter that you need to fill in the notebook is \"**your_name**\". It is the \"UserName\" you filled as a parameter when you setup the Cloudformation pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_alias = \"...\" # the name you used when creating the CF stacks, e.g. \"mia\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring the Container Image\n",
    "\n",
    "Let's start defining the hyperparameters and other attributes. Here the hyperparameters are randomly generated for demo purpose, you can leverage Sagemaker Autopilot for HPO (hyperparameters optimizaing). Here is a quick [demo](https://sagemaker-examples.readthedocs.io/en/latest/autopilot/custom-feature-selection/Feature_selection_autopilot.html#Set-up-and-kick-off-autopilot-job) on how to do that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246618743249.dkr.ecr.us-west-2.amazonaws.com/sagemaker-xgboost:1.0-1-cpu-py3\n"
     ]
    }
   ],
   "source": [
    "use_xgboost_builtin=True\n",
    "\n",
    "sts_client = boto3.client(\"sts\")\n",
    "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "region = boto3.session.Session().region_name\n",
    "model_prefix='iris-model'\n",
    "training_image = None\n",
    "hyperparameters = None\n",
    "\n",
    "if use_xgboost_builtin: \n",
    "    training_image = sagemaker.image_uris.retrieve('xgboost', boto3.Session().region_name, version='1.0-1')\n",
    "    hyperparameters = {\n",
    "        \"alpha\": 0.42495142279951414,\n",
    "        \"eta\": 0.4307531922567607,\n",
    "        \"gamma\": 1.8028358018081714,\n",
    "        \"max_depth\": 10,\n",
    "        \"min_child_weight\": 5.925133573560345,\n",
    "        \"num_class\": 3,\n",
    "        \"num_round\": 30,\n",
    "        \"objective\": \"multi:softmax\",\n",
    "        \"reg_lambda\": 10,\n",
    "        \"silent\": 0,\n",
    "    }\n",
    "else:\n",
    "    training_image = '{}.dkr.ecr.{}.amazonaws.com/{}:latest'.format(account_id, region, model_prefix)\n",
    "    hyperparameters = {\n",
    "        \"max_depth\": 11,\n",
    "        \"n_jobs\": 5,\n",
    "        \"n_estimators\": 120\n",
    "    }\n",
    "print(training_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Parameters\n",
    "\n",
    "Here, we define the parameters for training and deploying Lambda functions: `mlops-op-training` and `mlops-op-deployment`. You can check the details of functions in [**Lambda**](https://console.aws.amazon.com/lambda/home?region=us-west-2#/functions) console.\n",
    "\n",
    "#### Defining `training_params`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "roleArn = \"arn:aws:iam::{}:role/{}\".format(account_id, your_alias)\n",
    "timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "\n",
    "# Specify SageMaker Training job\n",
    "job_name = model_prefix + timestamp\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "training_params = {}\n",
    "\n",
    "# Here we set the reference for the Image Classification Docker image, stored on ECR (https://aws.amazon.com/pt/ecr/)\n",
    "training_params[\"AlgorithmSpecification\"] = {\n",
    "    \"TrainingImage\": training_image,\n",
    "    \"TrainingInputMode\": \"File\"\n",
    "}\n",
    "\n",
    "# The IAM role with all the permissions given to Sagemaker\n",
    "training_params[\"RoleArn\"] = roleArn\n",
    "\n",
    "# Here Sagemaker will store the final trained model\n",
    "training_params[\"OutputDataConfig\"] = {\n",
    "    \"S3OutputPath\": 's3://{}/{}'.format(sagemaker_session.default_bucket(), model_prefix)\n",
    "}\n",
    "\n",
    "# This is the config of the instance that will execute the training\n",
    "training_params[\"ResourceConfig\"] = {\n",
    "    \"InstanceCount\": 1,\n",
    "    \"InstanceType\": \"ml.m4.xlarge\",\n",
    "    \"VolumeSizeInGB\": 30\n",
    "}\n",
    "\n",
    "# The job name. You'll see this name in the Jobs section of the Sagemaker's console\n",
    "training_params[\"TrainingJobName\"] = job_name\n",
    "\n",
    "for i in hyperparameters:\n",
    "    hyperparameters[i] = str(hyperparameters[i])\n",
    "    \n",
    "# Here you will configure the hyperparameters used for training your model.\n",
    "training_params[\"HyperParameters\"] = hyperparameters\n",
    "\n",
    "# Training timeout\n",
    "training_params[\"StoppingCondition\"] = {\n",
    "    \"MaxRuntimeInSeconds\": 360000\n",
    "}\n",
    "\n",
    "# The algorithm currently only supports fully replicated model (where data is copied onto each machine)\n",
    "training_params[\"InputDataConfig\"] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we set training and validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for training\n",
    "training_params[\"InputDataConfig\"].append({\n",
    "    \"ChannelName\": \"train\",\n",
    "    \"DataSource\": {\n",
    "        \"S3DataSource\": {\n",
    "            \"S3DataType\": \"S3Prefix\",\n",
    "            \"S3Uri\": 's3://{}/{}/input/train'.format(\n",
    "                sagemaker_session.default_bucket(), \n",
    "                model_prefix),\n",
    "            \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "        }\n",
    "    },\n",
    "    \"ContentType\": \"text/csv\",\n",
    "    \"CompressionType\": \"None\"\n",
    "})\n",
    "\n",
    "# Parameters for tuning\n",
    "training_params[\"InputDataConfig\"].append({\n",
    "    \"ChannelName\": \"validation\",\n",
    "    \"DataSource\": {\n",
    "        \"S3DataSource\": {\n",
    "            \"S3DataType\": \"S3Prefix\",\n",
    "            \"S3Uri\": 's3://{}/{}/input/validation'.format(\n",
    "                sagemaker_session.default_bucket(), \n",
    "                model_prefix),\n",
    "            \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "        }\n",
    "    },\n",
    "    \"ContentType\": \"text/csv\",\n",
    "    \"CompressionType\": \"None\"\n",
    "})\n",
    "\n",
    "# Tags\n",
    "training_params[\"Tags\"] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining `deployment_params`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_params = {\n",
    "    \"EndpointPrefix\": model_prefix,\n",
    "    \"DevelopmentEndpoint\": {\n",
    "        # we want to enable the endpoint monitoring\n",
    "        \"InferenceMonitoring\": True,\n",
    "        # we will collect 100% of all the requests/predictions\n",
    "        \"InferenceMonitoringSampling\": 100,\n",
    "        \"InferenceMonitoringOutputBucket\": 's3://{}/{}/monitoring/dev'.format(\n",
    "            sagemaker_session.default_bucket(), model_prefix),\n",
    "        # we don't want to enable A/B tests in development\n",
    "        \"ABTests\": False,\n",
    "        # we'll use a basic instance for testing purposes\n",
    "        \"InstanceType\": \"ml.m4.xlarge\", \n",
    "        \"InitialInstanceCount\": 1,\n",
    "        # we don't want high availability/escalability for development\n",
    "        \"AutoScaling\": None\n",
    "    },\n",
    "    \"ProductionEndpoint\": {\n",
    "        # we want to enable the endpoint monitoring\n",
    "        \"InferenceMonitoring\": True,\n",
    "        # we will collect 100% of all the requests/predictions\n",
    "        \"InferenceMonitoringSampling\": 100,\n",
    "        \"InferenceMonitoringOutputBucket\": 's3://{}/{}/monitoring/prd'.format(\n",
    "            sagemaker_session.default_bucket(), model_prefix),\n",
    "        # we want to do A/B tests in production\n",
    "        \"ABTests\": True,\n",
    "        # we'll use a better instance for production. CPU optimized\n",
    "        \"InstanceType\": \"ml.m4.xlarge\", # \"ml.m4.xlarge\", # \n",
    "        \"InitialInstanceCount\": 1,\n",
    "        \"InitialVariantWeight\": 0.1,\n",
    "        # we want elasticity. at minimum 2 instances to support the endpoint and at maximum 10\n",
    "        # we'll use a threshold of 200 predictions per instance to start adding new instances or remove them\n",
    "        \"AutoScaling\": {\n",
    "            \"MinCapacity\": 1,\n",
    "            \"MaxCapacity\": 10,\n",
    "            \"TargetValue\": 200.0,\n",
    "            \"ScaleInCooldown\": 30,\n",
    "            \"ScaleOutCooldown\": 60,\n",
    "            \"PredefinedMetricType\": \"SageMakerVariantInvocationsPerInstance\"\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing and uploading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 174 ms, sys: 8.56 ms, total: 182 ms\n",
      "Wall time: 574 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sagemaker_session = sagemaker.Session()\n",
    "iris = datasets.load_iris()\n",
    "prefix='mlops/iris'\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    iris.data, iris.target, test_size=0.33, random_state=42, stratify=iris.target)\n",
    "\n",
    "np.savetxt(\"iris_train.csv\", np.column_stack((y_train, X_train)), delimiter=\",\", fmt='%0.3f')\n",
    "np.savetxt(\"iris_test.csv\", np.column_stack((y_test, X_test)), delimiter=\",\", fmt='%0.3f')\n",
    "\n",
    "# Upload the dataset to an S3 bucket\n",
    "input_train = sagemaker_session.upload_data(path='iris_train.csv', key_prefix='%s/input/train' % model_prefix)\n",
    "input_test = sagemaker_session.upload_data(path='iris_test.csv', key_prefix='%s/input/validation' % model_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activating the Pipeline \n",
    "\n",
    "Alright! Now it's time to start the end-to-end training/deployment process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucket_name : mlops-us-west-2-578778777738\n",
      "key_name : training_jobs/iris-model-new/trainingjob.zip\n"
     ]
    }
   ],
   "source": [
    "s3 = boto3.client('s3')\n",
    "sts_client = boto3.client(\"sts\")\n",
    "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "\n",
    "bucket_name = \"mlops-%s-%s\" % (region, account_id)\n",
    "print(\"bucket_name : {}\".format(bucket_name))\n",
    "key_name = \"training_jobs/%s/trainingjob.zip\" % model_prefix\n",
    "print(\"key_name : {}\".format(key_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's activate our pipeline by putting the training data into the [**S3 bucket**](https://s3.console.aws.amazon.com/s3/buckets/?region=us-west-2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '31E33RXX7F3K7H1J',\n",
       "  'HostId': '/G13IywwFoXidK0u7SSd79j/k0Z0mveTYaBiEvzKcMznE+5/h8M1jbXPMIjhuvdBJD/+1li6nnI=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': '/G13IywwFoXidK0u7SSd79j/k0Z0mveTYaBiEvzKcMznE+5/h8M1jbXPMIjhuvdBJD/+1li6nnI=',\n",
       "   'x-amz-request-id': '31E33RXX7F3K7H1J',\n",
       "   'date': 'Mon, 04 Apr 2022 02:08:21 GMT',\n",
       "   'x-amz-version-id': 'Z8xzRS_ut1pwCufZXLqBrKkeIB3b4tmf',\n",
       "   'etag': '\"3cf990ef87c5d23472ac63a4cf3435e7\"',\n",
       "   'server': 'AmazonS3',\n",
       "   'content-length': '0'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"3cf990ef87c5d23472ac63a4cf3435e7\"',\n",
       " 'VersionId': 'Z8xzRS_ut1pwCufZXLqBrKkeIB3b4tmf'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip_buffer = io.BytesIO()\n",
    "with zipfile.ZipFile(zip_buffer, 'a') as zf:\n",
    "    zf.writestr('trainingjob.json', json.dumps(training_params))\n",
    "    zf.writestr('deployment.json', json.dumps(deployment_params))\n",
    "zip_buffer.seek(0)\n",
    "\n",
    "s3.put_object(Bucket=bucket_name, Key=key_name, Body=bytearray(zip_buffer.read()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building ...\n",
    "\n",
    "While the pipeline is built automatically, open the [CodePipeline](https://us-west-2.console.aws.amazon.com/codesuite/codepipeline/pipelines/iris-model-pipeline/view?region=us-west-2)  console to see the status of our building pipeline. It will build the codepipeline following `ProcessRequest`, `Train` and `DeployDev`.\n",
    "\n",
    "After around **20 minutes**, the pipeline is finished."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Lab\n",
    "\n",
    "Once all steps (`ProcessRequest`, `Train` and `DeployDev`) turn to green. Go to the next notebook to see how to approve the deployment in production and monitor your endpoint."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
